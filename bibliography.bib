
@incollection{alexanderReciprocalInteractionsComputational2015,
  title = {Reciprocal {{Interactions}} of {{Computational Modeling}} and {{Empirical Investigation}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Alexander, William H. and Brown, Joshua W.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {321--338},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_16},
  abstract = {Models in general, and computational neural models in particular, are useful to the extent they fulfill three aims, which roughly constitute a life cycle of a model. First, at birth, models must account for existing phenomena, and with mechanisms that are no more complicated than necessary. Second, at maturity, models must make strong, falsifiable predictions that can guide future experiments. Third, all models are by definition incomplete, simplified representations of the mechanisms in question, so they should provide a basis of inspiration to guide the next generation of model development, as new data challenge and force the field to move beyond the existing models. Thus the final part of the model life cycle is a dialectic of model properties and empirical challenge. In this phase, new experimental data test and refine the model, leading either to a revised model or perhaps the birth of a new model. In what follows, we provide an outline of how this life cycle has played out in a particular series of models of the dorsal anterior cingulate cortex (ACC).},
  file = {/Users/andrew/Dropbox/Zotero/Alexander_Brown2015/Alexander and Brown - 2015 - Reciprocal Interactions of Computational Modeling .pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Anterior cingulate cortex,Cognitive control,Computational neural model,Dialectic,Error likelihood,Performance monitoring,Reinforcement learning},
  langid = {english}
}

@incollection{ashbyIntroductionFMRI2015,
  title = {An {{Introduction}} to {{fMRI}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Ashby, F. Gregory},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {91--112},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_5},
  abstract = {Functional magnetic resonance imaging (fMRI) provides an opportunity to indirectly observe neural activity noninvasively in the human brain as it changes in near real time. Most fMRI experiments measure the blood oxygen-level dependent (BOLD) signal, which rises to a peak several seconds after a brain area becomes active. Several experimental designs are common in fMRI research. Block designs alternate periods in which subjects perform some task with periods of rest, whereas event-related designs present the subject with a set of discrete trials. After the fMRI experiment is complete, pre-processing analyses prepare the data for task-related analyses. The most popular task-related analysis uses the General Linear Model to correlate a predicted BOLD response with the observed activity in each brain region. Regions where this correlation is high are identified as task related. Connectivity analysis then tries to identify active regions that belong to the same functional network. In contrast, multivariate methods, such as independent component analysis and multi-voxel pattern analysis identify networks of event-related regions, rather than single regions, so they simultaneously address questions of functional connectivity.},
  file = {/Users/andrew/Dropbox/Zotero/Ashby2015/Ashby - 2015 - An Introduction to fMRI.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {BOLD response,fMRI,Functional connectivity analysis,General Linear Model,Hemodynamic response function,Multiple comparisons problem,Preprocessing},
  langid = {english}
}

@incollection{bogaczOptimalDecisionMaking2015,
  title = {Optimal {{Decision Making}} in the {{Cortico}}-{{Basal}}-{{Ganglia Circuit}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Bogacz, Rafal},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {291--302},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_14},
  abstract = {This chapter presents a model assuming that during decision making the cortico-basal-ganglia circuit computes probabilities that considered alternatives are correct, according to Bayes' theorem. The model suggests how the equation of Bayes' theorem is mapped onto the functional anatomy of a circuit involving the cortex, basal ganglia and thalamus. The chapter also describes the relationship of the model to other models of decision making and experimental data.},
  file = {/Users/andrew/Dropbox/Zotero/Bogacz2015/Bogacz - 2015 - Optimal Decision Making in the Cortico-Basal-Gangl.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Action selection,Basal ganglia,Decision making},
  langid = {english}
}

@incollection{borstUsingACTRCognitive2015,
  title = {Using the {{ACT}}-{{R Cognitive Architecture}} in {{Combination With fMRI Data}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Borst, Jelmer P. and Anderson, John R.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {339--352},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_17},
  abstract = {In this chapter we discuss how the ACT-R cognitive architecture can be used in combination with fMRI data. ACT-R is a cognitive architecture that can provide a description of the processes from perception through to action for a wide range of cognitive tasks. It has a computational implementation that can be used to create models of specific tasks, which yield exact predictions in the form of response times and accuracy measures. In the last decade, researchers have extended the predictive capabilities of ACT-R to fMRI data. Since ACT-R provides a model of all the components in task performance it can address brain-wide activation patterns. fMRI data can now be used to inform and constrain the architecture, and, on the other hand, the architecture can be used to interpret fMRI data in a principled manner. In the following sections we first introduce cognitive architectures, and ACT-R in particular. Then, on the basis of an example dataset, we explain how ACT-R can be used to create fMRI predictions. In the third and fourth section of this chapter we discuss two ways in which these predictions can be used: region-of-interest and model-based fMRI analysis, and how the results can be used to inform the architecture and to interpret fMRI data.},
  file = {/Users/andrew/Dropbox/Zotero/Borst_Anderson2015/Borst and Anderson - 2015 - Using the ACT-R Cognitive Architecture in Combinat.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {ACT-R,Cognitive Architecture,fMRI,Model-based fMRI,ROI analysis},
  langid = {english}
}

@article{burknerBrmsPackageBayesian2017,
  title = {Brms: {{An R Package}} for {{Bayesian Multilevel Models Using Stan}}},
  shorttitle = {Brms},
  author = {B\"urkner, Paul-Christian},
  date = {2017-08-29},
  journaltitle = {Journal of Statistical Software},
  volume = {80},
  pages = {1--28},
  issn = {1548-7660},
  doi = {10.18637/jss.v080.i01},
  file = {/Users/andrew/Dropbox/Zotero/Bürkner2017/Bürkner - 2017 - brms An R Package for Bayesian Multilevel Models .pdf;/Users/andrew/Zotero/storage/J9A47ZDN/v080i01.html},
  keywords = {Bayesian inference,MCMC,multilevel model,ordinal data,R,Stan},
  langid = {english},
  number = {1}
}

@article{chaterProbabilisticBiasesMeet2020,
  title = {Probabilistic {{Biases Meet}} the {{Bayesian Brain}}},
  author = {Chater, Nick and Zhu, Jian-Qiao and Spicer, Jake and Sundh, Joakim and Le\'on-Villagr\'a, Pablo and Sanborn, Adam},
  date = {2020-10-01},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {29},
  pages = {506--512},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/0963721420954801},
  abstract = {In Bayesian cognitive science, the mind is seen as a spectacular probabilistic-inference machine. But judgment and decision-making (JDM) researchers have spent half a century uncovering how dramatically and systematically people depart from rational norms. In this article, we outline recent research that opens up the possibility of an unexpected reconciliation. The key hypothesis is that the brain neither represents nor calculates with probabilities but approximates probabilistic calculations by drawing samples from memory or mental simulation. Sampling models diverge from perfect probabilistic calculations in ways that capture many classic JDM findings, which offers the hope of an integrated explanation of classic heuristics and biases, including availability, representativeness, and anchoring and adjustment.},
  file = {/Users/andrew/Dropbox/Zotero/Chater et al/2020/Chater et al. - 2020 - Probabilistic Biases Meet the Bayesian Brain.pdf},
  keywords = {Bayesian inference,heuristics and biases,judgment and decision-making,probability,sampling},
  langid = {english},
  number = {5}
}

@report{devezerCaseFormalMethodology2020,
  title = {The Case for Formal Methodology in Scientific Reform},
  author = {Devezer, Berna and Navarro, Danielle J. and Vandekerckhove, Joachim and Buzbas, Erkan Ozge},
  date = {2020-04-28},
  institution = {{Scientific Communication and Education}},
  doi = {10.1101/2020.04.26.048306},
  abstract = {Abstract           Current attempts at methodological reform in sciences come in response to an overall lack of rigor in methodological and scientific practices in experimental sciences. However, most methodological reform attempts suffer from similar mistakes and over-generalizations to the ones they aim to address. We argue that this can be attributed in part to lack of formalism and first principles. Considering the costs of allowing false claims to become canonized, we argue for formal statistical rigor and scientific nuance in methodological reform. To attain this rigor and nuance, we propose a five-step formal approach for solving methodological problems. To illustrate the use and benefits of such formalism, we present a formal statistical analysis of three popular claims in the metascientific literature: (a) that reproducibility is the cornerstone of science; (b) that data must not be used twice in any analysis; and (c) that exploratory projects imply poor statistical practice. We show how our formal approach can inform and shape debates about such methodological claims.},
  file = {/Users/andrew/Zotero/storage/P54G956I/Devezer et al. - 2020 - The case for formal methodology in scientific refo.pdf},
  langid = {english},
  type = {preprint}
}

@incollection{ditterichDistinguishingModelsPerceptual2015,
  title = {Distinguishing {{Between Models}} of {{Perceptual Decision Making}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Ditterich, Jochen},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {277--290},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_13},
  abstract = {Mathematical models are a useful tool for gaining insight into mechanisms of decision making. However, like other scientific methods, its application is not without pitfalls. This chapter demonstrates that it can be difficult to distinguish between alternative models and it illustrates that a model-based approach benefits from the availability of a rich dataset that provides sufficient constraints. Ideally, the dataset is not only comprised of behavioral data, but also contains neural data that provide information about the internal processing. The chapter focuses on two examples taken from perceptual decision making. In one case, information about response time distributions is used to reject a model that is otherwise consistent with accuracy data and mean response times. In the other case, only the availability of neural data allows a distinction between two alternative models that are both consistent with the behavioral data.},
  file = {/Users/andrew/Dropbox/Zotero/Ditterich2015/Ditterich - 2015 - Distinguishing Between Models of Perceptual Decisi.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Choice,Feedback inhibition,Feedforward inhibition,Parietal cortex,Perceptual decision making,Response time,Stochastic integration,Time-variant},
  langid = {english}
}

@article{doornBayesFactorsMixed2021a,
  title = {Bayes {{Factors}} for {{Mixed Models}}},
  author = {family=Doorn, given=Johnny, prefix=van, useprefix=false and Aust, Frederik and Haaf, Julia M. and Stefan, Angelika and Wagenmakers, Eric-Jan},
  date = {2021-02-22T12:02:03},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/y65h8},
  abstract = {Although Bayesian mixed models are increasingly popular for data analysis in psychology and other fields, there remains considerable ambiguity on the most appropriate Bayes factor hypothesis test to quantify the degree to which the data support the presence or absence of an experimental effect. Specifically, different choices for both the null model and the alternative model are possible, and each choice constitutes a different definition of an effect resulting in a different test outcome. We outline the common approaches and focus on the impact of aggregation, the effect of measurement error, the choice of prior distribution, and the detection of interactions. For concreteness, three example scenarios showcase how seemingly innocuous choices can lead to dramatic differences in statistical evidence. We hope this work will facilitate a more explicit discussion about best practices in Bayes factor hypothesis testing in mixed models.},
  file = {/Users/andrew/Dropbox/Zotero/Doorn et al/2021/Doorn et al. - 2021 - Bayes Factors for Mixed Models2.pdf},
  keywords = {Bayes factors,Mixed effects,Mixed models,Quantitative Methods,Random effects,Social and Behavioral Sciences,Statistical Methods}
}

@article{etzHowBecomeBayesian2016,
  title = {How to Become a {{Bayesian}} in Eight Easy Steps: {{An}} Annotated Reading List},
  shorttitle = {How to Become a {{Bayesian}} in Eight Easy Steps},
  author = {Etz, Alexander and Gronau, Quentin Frederik and Dablander, Fabian and Edelsbrunner, Peter and Baribault, Beth},
  date = {2016-08-15T20:41:08},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/ph6sw},
  abstract = {In this guide, we present a reading list to serve as a concise introduction to Bayesian data analysis. The introduction is geared toward reviewers, editors, and interested researchers who are new to Bayesian statistics. We provide commentary for eight recommended sources, which together cover the theoretical and practical cornerstones of Bayesian statistics in psychology and related sciences.},
  file = {/Users/andrew/Dropbox/Zotero/Etz et al/2016/Etz et al. - 2016 - How to become a Bayesian in eight easy steps An a.pdf},
  keywords = {Bayes Factor,Bayesian Inference,Bayesian Statistics,Posterior Probability,psyarxiv,Quantitative Methods,Social and Behavioral Sciences,Theory and Philosophy of Science}
}

@article{etzIntroductionBayesianInference2018,
  title = {Introduction to {{Bayesian Inference}} for {{Psychology}}},
  author = {Etz, Alexander and Vandekerckhove, Joachim},
  date = {2018-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {25},
  pages = {5--34},
  issn = {1531-5320},
  doi = {10.3758/s13423-017-1262-3},
  abstract = {We introduce the fundamental tenets of Bayesian inference, which derive from two basic laws of probability theory. We cover the interpretation of probabilities, discrete and continuous versions of Bayes' rule, parameter estimation, and model comparison. Using seven worked examples, we illustrate these principles and set up some of the technical background for the rest of this special issue of Psychonomic Bulletin \& Review. Supplemental material is available via https://osf.io/wskex/.},
  file = {/Users/andrew/Dropbox/Zotero/Etz_Vandekerckhove/2018/Etz and Vandekerckhove - 2018 - Introduction to Bayesian Inference for Psychology.pdf},
  langid = {english},
  number = {1}
}

@incollection{farrellIntroductionCognitiveModeling2015,
  title = {An {{Introduction}} to {{Cognitive Modeling}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Farrell, Simon and Lewandowsky, Stephan},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {3--24},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_1},
  abstract = {We provide a tutorial on the basic attributes of computational cognitive models\textemdash models that are formulated as a set of mathematical equations or as a computer simulation. We first show how models can generate complex behavior and novel insights from very simple underlying assumptions about human cognition. We survey the different classes of models, from description to explanation, and present examples of each class. We then illustrate the reasons why computational models are preferable to purely verbal means of theorizing. For example, we show that computational models help theoreticians overcome the limitations of human cognition, thereby enabling us to create coherent and plausible accounts of how we think or remember and guard against subtle theoretical errors. Models can also measure latent constructs and link them to individual differences, which would escape detection if only the raw data were considered. We conclude by reviewing some open challenges.},
  file = {/Users/andrew/Dropbox/Zotero/Farrell_Lewandowsky2015/Farrell and Lewandowsky - 2015 - An Introduction to Cognitive Modeling.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Agent-based modelling,Computational models,Model comparison,Necessity,Parameter interpretation,Practice,Scientific reasoning},
  langid = {english}
}

@online{FirstLessonBayesian,
  title = {A {{First Lesson}} in {{Bayesian Inference}}},
  url = {http://lmpp10e-mucesm.srv.mwn.de:3838/felix/BayesLessons/BayesianLesson1.Rmd},
  urldate = {2021-03-01},
  file = {/Users/andrew/Zotero/storage/FHZCVJDM/BayesianLesson1.html}
}

@incollection{forstmannIntroductionHumanBrain2015,
  title = {An {{Introduction}} to {{Human Brain Anatomy}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Forstmann, Birte U. and Keuken, Max C. and Alkemade, Anneke},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {71--89},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_4},
  abstract = {This tutorial chapter provides an overview of the human brain anatomy. Knowledge of brain anatomy is fundamental to our understanding of cognitive processes in health and disease; moreover, anatomical constraints are vital for neurocomputational models and can be important for psychological theorizing as well. The main challenge in understanding brain anatomy is to integrate the different levels of description ranging from molecules to macroscopic brain networks. This chapter contains three main sections. The first section provides a brief introduction to the neuroanatomical nomenclature. The second section provides an introduction to the different levels of brain anatomy and describes commonly used atlases for the visualization of functional imaging data. The third section provides a concrete example of how human brain structure relates to performance.},
  file = {/Users/andrew/Dropbox/Zotero/Forstmann et al2015/Forstmann et al. - 2015 - An Introduction to Human Brain Anatomy.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Connectional neuroanatomy,functional MRI,Neuroanatomical atlases,Sectional neuroanatomy,Structural MRI,Structure-function relationships,Ultra high resolution MRI},
  langid = {english}
}

@incollection{forstmannModelBasedCognitiveNeuroscience2015,
  title = {Model-{{Based Cognitive Neuroscience}}: {{A Conceptual Introduction}}},
  shorttitle = {Model-{{Based Cognitive Neuroscience}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {139--156},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_7},
  abstract = {This tutorial chapter shows how the separate fields of mathematical psychology and cognitive neuroscience can interact to their mutual benefit. Historically, the field of mathematical psychology is mostly concerned with formal theories of behavior, whereas cognitive neuroscience is mostly concerned with empirical measurements of brain activity. Despite these superficial differences in method, the ultimate goal of both disciplines is the same: to understand the workings of human cognition. In recognition of this common purpose, mathematical psychologists have recently started to apply their models in cognitive neuroscience, and cognitive neuroscientists have borrowed and extended key ideas that originated from mathematical psychology. This chapter consists of three main sections: the first describes the field of mathematical psychology, the second describes the field of cognitive neuroscience, and the third describes their recent combination: model-based cognitive neuroscience.},
  file = {/Users/andrew/Dropbox/Zotero/Forstmann_Wagenmakers2015/Forstmann and Wagenmakers - 2015 - Model-Based Cognitive Neuroscience A Conceptual I.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Blood Oxygenation Level Dependent,Blood Oxygenation Level Dependent Signal,Cognitive Neuroscience,Drift Rate,Mathematical Psychology},
  langid = {english}
}

@incollection{frankLinkingLevelsComputation2015,
  title = {Linking {{Across Levels}} of {{Computation}} in {{Model}}-{{Based Cognitive Neuroscience}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Frank, Michael J.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {159--177},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_8},
  abstract = {Computational approaches to cognitive neuroscience encompass multiple levels of analysis, from detailed biophysical models of neural activity to abstract algorithmic or normative models of cognition, with several levels in between. Despite often strong opinions on the `right' level of modeling, there is no single panacea: attempts to link biological with higher level cognitive processes require a multitude of approaches. Here I argue that these disparate approaches should not be viewed as competitive, nor should they be accessible to only other researchers already endorsing the particular level of modeling. Rather, insights gained from one level of modeling should inform modeling endeavors at the level above and below it. One way to achieve this synergism is to link levels of modeling by quantitatively fitting the behavioral outputs of detailed mechanistic models with higher level descriptions. If the fits are reasonable (e.g., similar to those achieved when applying high level models to human behavior), one can then derive plausible links between mechanism and computation. Model-based cognitive neuroscience approaches can then be employed to manipulate or measure neural function motivated by the candidate mechanisms, and to test whether these are related to high level model parameters. I describe several examples of this approach in the domain of reward-based learning, cognitive control, and decision making and show how neural and algorithmic models have each informed or refined the other.},
  file = {/Users/andrew/Dropbox/Zotero/Frank2015/Frank - 2015 - Linking Across Levels of Computation in Model-Base.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Algorithms,Basal ganglia,Computational models,Decision making,Dopamine,Neural networks,Prefrontal cortex,Reinforcement learning},
  langid = {english}
}

@book{gelmanBayesianDataAnalysis2014,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew},
  date = {2014},
  edition = {Third edition},
  publisher = {{CRC Press}},
  location = {{Boca Raton}},
  abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
  isbn = {978-1-4398-4095-5},
  keywords = {Bayesian statistical decision theory,MATHEMATICS / Probability & Statistics / General},
  pagetotal = {661},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science}
}

@article{gigerenzerMindlessStatistics2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  date = {2004-11},
  journaltitle = {The Journal of Socio-Economics},
  volume = {33},
  pages = {587--606},
  issn = {10535357},
  doi = {10.1016/j.socec.2004.09.033},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the ``null ritual'' consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  file = {/Users/andrew/Dropbox/Zotero/Gigerenzer2004/Gigerenzer - 2004 - Mindless statistics.pdf},
  langid = {english},
  number = {5}
}

@article{gigerenzerStatisticalRitualsReplication2018a,
  title = {Statistical {{Rituals}}: {{The Replication Delusion}} and {{How We Got There}}},
  shorttitle = {Statistical {{Rituals}}},
  author = {Gigerenzer, Gerd},
  date = {2018-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {198--218},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918771329},
  abstract = {The ``replication crisis'' has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers' widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The ``null ritual,'' unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 \textendash{} p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20\% of the faculty teaching statistics in psychology, 39\% of the professors and lecturers, and 66\% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 \textendash{} p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56\%\textendash 97\%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as ``significant'' or ``not significant.''},
  file = {/Users/andrew/Dropbox/Zotero/Gigerenzer/2018/Gigerenzer - 2018 - Statistical Rituals The Replication Delusion and .pdf},
  keywords = {illusion of certainty,null ritual,p value,p-hacking,replication},
  langid = {english},
  number = {2}
}

@article{guestHowComputationalModeling2021,
  title = {How {{Computational Modeling Can Force Theory Building}} in {{Psychological Science}}},
  author = {Guest, Olivia and Martin, Andrea E.},
  date = {2021-01-22},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  pages = {1745691620970585},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620970585},
  abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined\textemdash what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
  file = {/Users/andrew/Zotero/storage/SVPEKCYE/Guest and Martin - 2021 - How Computational Modeling Can Force Theory Buildi.pdf},
  keywords = {computational model,open science,scientific inference,theoretical psychology},
  langid = {english}
}

@article{hainesLearningReliabilityParadox2020a,
  title = {Learning from the {{Reliability Paradox}}: {{How Theoretically Informed Generative Models Can Advance}} the {{Social}}, {{Behavioral}}, and {{Brain Sciences}}},
  shorttitle = {Learning from the {{Reliability Paradox}}},
  author = {Haines, Nathaniel and Kvam, Peter D. and Irving, Louis H. and Smith, Colin and Beauchaine, Theodore P. and Pitt, Mark A. and Ahn, Woo-Young and Turner, Brandon},
  date = {2020-08-24T13:56:49},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/xr7y3},
  abstract = {Behavioral tasks (e.g., Stroop task) that produce replicable group-level effects (e.g., Stroop effect) often fail to reliably capture individual differences between participants (e.g., low test-retest reliability). This ``reliability paradox'' has led many researchers to conclude that most behavioral tasks cannot be used to develop and advance theories of individual differences. However, these conclusions are derived from statistical models that provide only superficial summary descriptions of behavioral data, thereby ignoring theoretically-relevant data-generating mechanisms that underly individual-level behavior. More generally, such descriptive methods lack the flexibility to test and develop increasingly complex theories of individual differences. To resolve this theory-description gap, we present generative modeling approaches, which involve using background knowledge to specify how behavior is generated at the individual level, and in turn how the distributions of individual-level mechanisms are characterized at the group level\textemdash all in a single joint model. Generative modeling shifts our focus away from estimating descriptive statistical ``effects'' toward estimating psychologically meaningful parameters, while simultaneously accounting for measurement error that would otherwise attenuate individual difference correlations. Using simulations and empirical data from the Implicit Association Test and Stroop, Flanker, Posner Cueing, and Delay Discounting tasks, we demonstrate how generative models yield (1) higher test-retest reliability estimates, and (2) more theoretically informative parameter estimates relative to traditional statistical approaches. Our results reclaim optimism regarding the utility of behavioral paradigms for testing and advancing theories of individual differences, and emphasize the importance of formally specifying and checking model assumptions to reduce theory-description gaps and facilitate principled theory development.},
  file = {/Users/andrew/Dropbox/Zotero/Haines et al/2020/Haines et al. - 2020 - Learning from the Reliability Paradox How Theoret2.pdf},
  keywords = {Bayesian analysis,Clinical Psychology,Cognitive Psychology,Generative modeling,Implicit attitudes,Impulsivity,Individual differences,Measurement error,Meta-science,Quantitative Methods,Reliability,Self-control,Social and Behavioral Sciences,Social and Personality Psychology,Theory and Philosophy of Science,Theory development}
}

@incollection{heathcoteIntroductionGoodPractices2015,
  title = {An {{Introduction}} to {{Good Practices}} in {{Cognitive Modeling}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Heathcote, Andrew and Brown, Scott D. and Wagenmakers, Eric-Jan},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {25--48},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_2},
  abstract = {Cognitive modeling can provide important insights into the underlying causes of behavior, but the validity of those insights rests on careful model development and checking. We provide guidelines on five important aspects of the practice of cognitive modeling: parameter recovery, testing selective influence of experimental manipulations on model parameters, quantifying uncertainty in parameter estimates, testing and displaying model fit, and selecting among different model parameterizations and types of models. Each aspect is illustrated with examples.},
  file = {/Users/andrew/Dropbox/Zotero/Heathcote et al2015/Heathcote et al. - 2015 - An Introduction to Good Practices in Cognitive Mod.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Cognition,Model,Model selection,Parameter estimation,Quantitative,Simulation study,Theory},
  langid = {english}
}

@article{hoekstraRobustMisinterpretationConfidence2014,
  title = {Robust Misinterpretation of Confidence Intervals},
  author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  date = {2014-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {21},
  pages = {1157--1164},
  issn = {1531-5320},
  doi = {10.3758/s13423-013-0572-3},
  abstract = {Null hypothesis significance testing (NHST) is undoubtedly the most common inferential technique used to justify claims in the social sciences. However, even staunch defenders of NHST agree that its outcomes are often misinterpreted. Confidence intervals (CIs) have frequently been proposed as a more useful alternative to NHST, and their use is strongly encouraged in the APA Manual. Nevertheless, little is known about how researchers interpret CIs. In this study, 120 researchers and 442 students\textemdash all in the field of psychology\textemdash were asked to assess the truth value of six particular statements involving different interpretations of a CI. Although all six statements were false, both researchers and students endorsed, on average, more than three statements, indicating a gross misunderstanding of CIs. Self-declared experience with statistics was not related to researchers' performance, and, even more surprisingly, researchers hardly outperformed the students, even though the students had not received any education on statistical inference whatsoever. Our findings suggest that many researchers do not know the correct interpretation of a CI. The misunderstandings surrounding p-values and CIs are particularly unfortunate because they constitute the main tools by which psychologists draw conclusions from data.},
  file = {/Users/andrew/Dropbox/Zotero/Hoekstra et al/2014/Hoekstra et al. - 2014 - Robust misinterpretation of confidence intervals.pdf},
  langid = {english},
  number = {5}
}

@incollection{kokPredictiveCodingSensory2015,
  title = {Predictive {{Coding}} in {{Sensory Cortex}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Kok, Peter and family=Lange, given=Floris P., prefix=de, useprefix=true},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {221--244},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_11},
  abstract = {In recent years, predictive coding has become an increasingly influential model of how the brain processes sensory information. Predictive coding theories state that the brain is constantly trying to predict the inputs it receives, and each region in the cortical sensory hierarchy represents both these predictions and the mismatch between predictions and input (prediction error). In this chapter, we review the extant empirical evidence for this theory, as well as discuss recent theoretical advances. We find that predictive coding provides a good explanation for many phenomena observed in perception, and generates testable hypotheses. Furthermore, we suggest possible avenues for further empirical testing and for broadening the perspective of the role predictive coding may play in cognition.},
  file = {/Users/andrew/Dropbox/Zotero/Kok_de Lange2015/Kok and de Lange - 2015 - Predictive Coding in Sensory Cortex.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Expectation,fMRI,Perception,Perceptual inference,Prediction,Predictive coding},
  langid = {english}
}

@article{kordingBayesianDecisionTheory2006,
  title = {Bayesian Decision Theory in Sensorimotor Control},
  author = {K\"ording, Konrad P. and Wolpert, Daniel M.},
  date = {2006-07},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  pages = {319--326},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.05.003},
  file = {/Users/andrew/Dropbox/Zotero/Körding_Wolpert/2006/Körding and Wolpert - 2006 - Bayesian decision theory in sensorimotor control.pdf},
  langid = {english},
  number = {7}
}

@article{kordingBayesianIntegrationSensorimotor2004,
  title = {Bayesian Integration in Sensorimotor Learning},
  author = {K\"ording, Konrad P. and Wolpert, Daniel M.},
  date = {2004-01-15},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {427},
  pages = {244--247},
  issn = {1476-4687},
  doi = {10.1038/nature02169},
  abstract = {When we learn a new motor skill, such as playing an approaching tennis ball, both our sensors and the task possess variability. Our sensors provide imperfect information about the ball's velocity, so we can only estimate it. Combining information from multiple modalities can reduce the error in this estimate. On a longer time scale, not all velocities are a priori equally probable, and over the course of a match there will be a probability distribution of velocities. According to bayesian theory, an optimal estimate results from combining information about the distribution of velocities-the prior-with evidence from sensory feedback. As uncertainty increases, when playing in fog or at dusk, the system should increasingly rely on prior knowledge. To use a bayesian strategy, the brain would need to represent the prior distribution and the level of uncertainty in the sensory feedback. Here we control the statistical variations of a new sensorimotor task and manipulate the uncertainty of the sensory feedback. We show that subjects internally represent both the statistical distribution of the task and their sensory uncertainty, combining them in a manner consistent with a performance-optimizing bayesian process. The central nervous system therefore employs probabilistic models during sensorimotor learning.},
  eprint = {14724638},
  eprinttype = {pmid},
  file = {/Users/andrew/Dropbox/Zotero/Körding_Wolpert/2004/Körding and Wolpert - 2004 - Bayesian integration in sensorimotor learning.pdf},
  keywords = {Bayes Theorem,Brain,Feedback,Female,Fingers,Humans,Learning,Male,Motor Skills,Movement,Normal Distribution,Photic Stimulation,Psychomotor Performance},
  langid = {english},
  number = {6971}
}

@article{kordingDecisionTheoryWhat2007,
  title = {Decision {{Theory}}: {{What}} "{{Should}}" the {{Nervous System Do}}?},
  shorttitle = {Decision {{Theory}}},
  author = {K\"ording, Konrad},
  date = {2007-10-26},
  journaltitle = {Science},
  volume = {318},
  pages = {606--610},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1142998},
  abstract = {The purpose of our nervous system is to allow us to successfully interact with our environment. This normative idea is formalized by decision theory that defines which choices would be most beneficial. We live in an uncertain world, and each decision may have many possible outcomes; choosing the best decision is thus complicated. Bayesian decision theory formalizes these problems in the presence of uncertainty and often provides compact models that predict observed behavior. With its elegant formalization of the problems faced by the nervous system, it promises to become a major inspiration for studies in neuroscience.},
  eprint = {17962554},
  eprinttype = {pmid},
  file = {/Users/andrew/Zotero/storage/3ZXA5PCS/606.html},
  langid = {english},
  number = {5850}
}

@article{kruschkeBayesianEstimationSupersedes2013,
  title = {Bayesian Estimation Supersedes the t Test.},
  author = {Kruschke, John K.},
  date = {2013},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {142},
  pages = {573--603},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0029146},
  abstract = {Bayesian estimation for 2 groups provides complete distributions of credible values for the effect size, group means and their difference, standard deviations and their difference, and the normality of the data. The method handles outliers. The decision rule can accept the null value (unlike traditional t tests) when certainty in the estimate is high (unlike Bayesian model comparison using Bayes factors). The method also yields precise estimates of statistical power for various research goals. The software and programs are free and run on Macintosh, Windows, and Linux platforms.},
  file = {/Users/andrew/Dropbox/Zotero/Kruschke2013/Kruschke - 2013 - Bayesian estimation supersedes the t test..pdf},
  langid = {english},
  number = {2}
}

@book{kruschkeDoingBayesianData2015,
  title = {Doing Bayesian Data Analysis (Second Edition)},
  author = {Kruschke, John},
  date = {2015},
  publisher = {{Academic Press}},
  location = {{Boston}},
  added-at = {2016-12-29T09:25:25.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/277f97f6f84d077790b702e30ed86be5f/becker},
  interhash = {5e4043e24f7f58a8de076d7956ca08ea},
  intrahash = {77f97f6f84d077790b702e30ed86be5f},
  keywords = {diss imported inthesis mixedtrails},
  timestamp = {2017-06-19T10:12:05.000+0200}
}

@article{lazarASAStatementPValues2016,
  title = {The {{ASA}}'s {{Statement}} on p-{{Values}}: {{Context}}, {{Process}}, and {{Purpose AU}}  - {{Wasserstein}}, {{Ronald L}}.},
  shorttitle = {The {{ASA}}'s {{Statement}} on p-{{Values}}},
  author = {Lazar, Nicole A.},
  date = {2016-04-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {70},
  pages = {129--133},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  file = {/Users/andrew/Zotero/storage/59NPBAY4/00031305.2016.html},
  number = {2}
}

@book{leeBayesianCognitiveModeling2014a,
  title = {Bayesian {{Cognitive Modeling}}: {{A Practical Course}}},
  shorttitle = {Bayesian {{Cognitive Modeling}}},
  author = {Lee, Michael D. and Wagenmakers, Eric-Jan},
  date = {2014},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9781139087759},
  abstract = {Bayesian inference has become a standard method of analysis in many fields of science. Students and researchers in experimental psychology and cognitive science, however, have failed to take full advantage of the new and exciting possibilities that the Bayesian approach affords. Ideal for teaching and self study, this book demonstrates how to do Bayesian modeling. Short, to-the-point chapters offer examples, exercises, and computer code (using WinBUGS or JAGS, and supported by Matlab and R), with additional support available online. No advance knowledge of statistics is required and, from the very start, readers are encouraged to apply and adjust Bayesian analyses by themselves. The book contains a series of chapters on parameter estimation and model selection, followed by detailed case studies from cognitive science. After working through this book, readers should be able to build their own Bayesian models, apply the models to their own data, and draw their own conclusions.},
  file = {/Users/andrew/Zotero/storage/63Z97PSD/B477C799F1DB4EBB06F4EBAFBFD2C28B.html},
  isbn = {978-1-107-01845-7}
}

@incollection{loganInhibitoryControlMind2015,
  title = {Inhibitory {{Control}} in {{Mind}} and {{Brain}}: {{The Mathematics}} and {{Neurophysiology}} of the {{Underlying Computation}}},
  shorttitle = {Inhibitory {{Control}} in {{Mind}} and {{Brain}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Logan, Gordon D. and Schall, Jeffrey D. and Palmeri, Thomas J.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {303--320},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_15},
  abstract = {We develop desiderata for a computational theory of response inhibition that links mathematical psychology with neuroscience. The theory must be explicit mathematically and computationally, and grounded in behavior and neurophysiology. The theory must provide quantitative accounts of complexities of behavior in response inhibition tasks and must predict the neural activity that underlies performance. We evaluate three current theories of response inhibition in the stop signal paradigm using these desiderata, and we find that one theory fulfills the desiderata better than the others.},
  file = {/Users/andrew/Dropbox/Zotero/Logan et al2015/Logan et al. - 2015 - Inhibitory Control in Mind and Brain The Mathemat.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Computational theory of response inhibition,Mathematical psychology,Neuroscience,Response inhibition},
  langid = {english}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical Rethinking: {{A}} Bayesian Course with Examples in r and Stan},
  author = {McElreath, R.},
  date = {2020},
  publisher = {{CRC Press}},
  url = {https://books.google.ch/books?id=Ie2vxQEACAAJ},
  isbn = {978-0-367-13991-9},
  lccn = {2019957006},
  series = {A Chapman \& Hall Book}
}

@article{navarroIfMathematicalPsychology2020,
  title = {If Mathematical Psychology Did Not Exist We Might Need to Invent It: {{A}} Comment on Theory Building in Psychology},
  shorttitle = {If Mathematical Psychology Did Not Exist We Might Need to Invent It},
  author = {Navarro, Danielle},
  date = {2020-03-16T20:56:43},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/ygbjp},
  abstract = {It is commonplace, when discussing the subject of psychological theory, to write articles from the assumption that psychology differs from physical sciences in that we have no theories that would support cumulative, incremental science. In this brief paper I discuss one counterexample, namely Shepard's (1987) law of generalization and the various Bayesian extensions that it inspired over the last three decades. Using Shepard's law as a running example I argue that psychological theory building is not a statistical problem; mathematical formalism is theoretically beneficial; measurement and theory have a complex relationship; rewriting old theory can yield new insights; and finally, that theoretical growth can drive empirical work. Though generally suggesting that the tools of mathematical psychology are valuable to the psychological theorist, the paper also comments on some limitations to this approach.},
  file = {/Users/andrew/Zotero/storage/VP6P5EU5/Navarro - 2020 - If mathematical psychology did not exist we might .pdf},
  keywords = {Cognitive Psychology,Concepts and Categories,Meta-science,Reasoning,Social and Behavioral Sciences,Theory and Philosophy of Science}
}

@article{navarroPersonalEssayBayes2020,
  title = {A Personal Essay on {{Bayes}} Factors},
  author = {Navarro, Danielle},
  date = {2020-12-07T06:46:53},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/nujy6},
  abstract = {This is an archived version of a blog post on Bayes factors. It is a personal reflection on some of the practical issues that one encounters when attempting to apply Bayes factors to difficult inference problems. The main message of the piece is real world inference is hard and that being too prescriptive about how statistics must be done is generally a recipe for disaster.},
  file = {/Users/andrew/Dropbox/Zotero/Navarro/2020/Navarro - 2020 - A personal essay on Bayes factors.pdf},
  keywords = {Bayes factors,Mathematical Psychology,Quantitative Methods,Social and Behavioral Sciences}
}

@incollection{oreillyBayesianModelsCognitive2015,
  title = {Bayesian {{Models}} in {{Cognitive Neuroscience}}: {{A Tutorial}}},
  shorttitle = {Bayesian {{Models}} in {{Cognitive Neuroscience}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {O'Reilly, Jill X. and Mars, Rogier B.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {179--197},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_9},
  abstract = {This chapter provides an introduction to Bayesian models and their application in cognitive neuroscience. The central feature of Bayesian models, as opposed to other classes of models, is that Bayesian models represent the beliefs of an observer as probability distributions, allowing them to integrate information while taking its uncertainty into account. In the chapter, we will consider how the probabilistic nature of Bayesian models makes them particularly useful in cognitive neuroscience. We will consider two types of tasks in which we believe a Bayesian approach is useful: optimal integration of evidence from different sources, and the development of beliefs about the environment given limited information (such as during learning). We will develop some detailed examples of Bayesian models to give the reader a taste of how the models are constructed and what insights they may be able to offer about participants' behavior and brain activity.},
  file = {/Users/andrew/Dropbox/Zotero/O’Reilly_Mars2015/O’Reilly and Mars - 2015 - Bayesian Models in Cognitive Neuroscience A Tutor.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Attention,Bayes,Bayesian modelling,Probability,Uncertainty},
  langid = {english}
}

@article{rouderBayesianTestsAccepting2009a,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  date = {2009-04-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {16},
  pages = {225--237},
  issn = {1531-5320},
  doi = {10.3758/PBR.16.2.225},
  abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
  file = {/Users/andrew/Dropbox/Zotero/Rouder et al2009/Rouder et al. - 2009 - Bayesian t tests for accepting and rejecting the n.pdf},
  keywords = {Akaike Information Criterion,Marginal Likelihood,Posterior Odds,Prior Standard Deviation,Subliminal Priming},
  langid = {english},
  number = {2}
}

@article{rousseletFewSimpleSteps2016,
  title = {A Few Simple Steps to Improve the Description of Group Results in Neuroscience},
  author = {Rousselet, Guillaume A. and Foxe, John J. and Bolam, J. Paul},
  date = {2016},
  journaltitle = {European Journal of Neuroscience},
  volume = {44},
  pages = {2647--2651},
  issn = {1460-9568},
  doi = {10.1111/ejn.13400},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ejn.13400},
  file = {/Users/andrew/Dropbox/Zotero/Rousselet et al/2016/Rousselet et al. - 2016 - A few simple steps to improve the description of g.pdf;/Users/andrew/Zotero/storage/HFQZRAIL/ejn.html},
  langid = {english},
  number = {9}
}

@report{rousseletReactionTimesOther2019,
  title = {Reaction Times and Other Skewed Distributions: Problems with the Mean and the Median},
  shorttitle = {Reaction Times and Other Skewed Distributions},
  author = {Rousselet, Guillaume and Wilcox, Rand R.},
  date = {2019-01-17T11:18:00},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/3y54r},
  abstract = {To summarise skewed (asymmetric) distributions, such as reaction times, typically the mean or the median are used as measures of central tendency. Using the mean might seem surprising, given that it provides a poor measure of central tendency for skewed distributions, whereas the median provides a better indication of the location of the bulk of the observations. However, the sample median is biased: with small sample sizes, it tends to overestimate the population median. This is not the case for the mean. Based on this observation, Miller (1988) concluded that ''sample medians must not be used to compare reaction times across experimental conditions when there are unequal numbers of trials in the conditions.'' Here we replicate and extend Miller (1988), and demonstrate that his conclusion was ill-advised for several reasons. First, the median's bias can be corrected using a percentile bootstrap bias correction. Second, a careful examination of the sampling distributions reveals that the sample median is median unbiased, whereas the mean is median biased when dealing with skewed distributions. That is, on average the sample mean estimates the population mean, but typically this is not the case. In addition, simulations of false and true positives in various situations show that no method dominates. Crucially, neither the mean nor the median are sufficient or even necessary to compare skewed distributions. Different questions require different methods and it would be unwise to use the mean or the median in all situations. Better tools are available to get a deeper understanding of how distributions differ: we illustrate the hierarchical shift function, a powerful alternative that relies on quantile estimation. All the code and data to reproduce the figures and analyses in the article are available online.},
  file = {/Users/andrew/Dropbox/Zotero/Rousselet_Wilcox/2019/Rousselet and Wilcox - 2019 - Reaction times and other skewed distributions pro.pdf},
  keywords = {bias,bootstrap,estimation,mean,median,Meta-science,quantile,Quantitative Methods,sampling,skewness,Social and Behavioral Sciences,Statistical Methods,trimmed mean}
}

@article{schootGentleIntroductionBayesian2014,
  title = {A {{Gentle Introduction}} to {{Bayesian Analysis}}: {{Applications}} to {{Developmental Research}}},
  shorttitle = {A {{Gentle Introduction}} to {{Bayesian Analysis}}},
  author = {family=Schoot, given=Rens, prefix=van de, useprefix=false and Kaplan, David and Denissen, Jaap and Asendorpf, Jens B. and Neyer, Franz J. and family=Aken, given=Marcel A. G., prefix=van, useprefix=false},
  date = {2014},
  journaltitle = {Child Development},
  volume = {85},
  pages = {842--860},
  issn = {1467-8624},
  doi = {10.1111/cdev.12169},
  abstract = {Bayesian statistical methods are becoming ever more popular in applied and fundamental research. In this study a gentle introduction to Bayesian analysis is provided. It is shown under what circumstances it is attractive to use Bayesian estimation, and how to interpret properly the results. First, the ingredients underlying Bayesian methods are introduced using a simplified example. Thereafter, the advantages and pitfalls of the specification of prior knowledge are discussed. To illustrate Bayesian methods explained in this study, in a second example a series of studies that examine the theoretical framework of dynamic interactionism are considered. In the Discussion the advantages and disadvantages of using Bayesian statistics are reviewed, and guidelines on how to report on Bayesian statistics are provided.},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.12169},
  file = {/Users/andrew/Dropbox/Zotero/Schoot et al/2014/Schoot et al. - 2014 - A Gentle Introduction to Bayesian Analysis Applic.pdf;/Users/andrew/Zotero/storage/A3HU67Y4/cdev.html},
  langid = {english},
  number = {3}
}

@incollection{singmannIntroductionMixedModels2019,
  title = {An {{Introduction}} to {{Mixed Models}} for {{Experimental Psychology}}},
  booktitle = {New {{Methods}} in {{Cognitive Psychology}}},
  author = {Singmann, Henrik and Kellen, David},
  editor = {Spieler, Daniel and Schumacher, Eric},
  date = {2019-10-28},
  edition = {1},
  pages = {4--31},
  publisher = {{Routledge}},
  doi = {10.4324/9780429318405-2},
  file = {/Users/andrew/Zotero/storage/SBC6A7LI/Singmann and Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf},
  isbn = {978-0-429-31840-5},
  langid = {english}
}

@incollection{smithIntroductionDiffusionModel2015,
  title = {An {{Introduction}} to the {{Diffusion Model}} of {{Decision Making}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Smith, Philip L. and Ratcliff, Roger},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {49--70},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_3},
  abstract = {The diffusion model assumes that two-choice decisions are made by accumulating successive samples of noisy evidence to a response criterion. The model has a pair of criteria that represent the amounts of evidence needed to make each response. The time taken to reach criterion determines the decision time and the criterion that is reached first determines the response. The model predicts choice probabilities and the distributions of response times for correct responses and errors as a function of experimental conditions such as stimulus discriminability, speed-accuracy instructions, and manipulations of relative stimulus frequency, which affect response bias. This chapter describes the main features of the model, including mathematical methods for obtaining response time predictions, methods for fitting it to experimental data, including alternative fitting criteria, and ways to represent the fit to multiple experimental conditions graphically in a compact way. The chapter concludes with a discussion of recent work in psychology that links evidence accumulation to processes of perception, attention, and memory, and in neuroscience, to neural firing rates in the oculomotor control system in monkeys performing saccade-to-target decision tasks.},
  file = {/Users/andrew/Dropbox/Zotero/Smith_Ratcliff2015/Smith and Ratcliff - 2015 - An Introduction to the Diffusion Model of Decision.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Choice probability,Decision-making,Diffusion process,Random walk,Response time},
  langid = {english}
}

@article{speckmanDeltaPlotsCoherent2008a,
  title = {Delta {{Plots}} and {{Coherent Distribution Ordering}}},
  author = {Speckman, Paul L and Rouder, Jeffrey N and Morey, Richard D and Pratte, Michael S},
  date = {2008-08},
  journaltitle = {The American Statistician},
  volume = {62},
  pages = {262--266},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/000313008X333493},
  file = {/Users/andrew/Dropbox/Zotero/Speckman et al/2008/Speckman et al. - 2008 - Delta Plots and Coherent Distribution Ordering2.pdf},
  langid = {english},
  number = {3}
}

@incollection{spragueUsingHumanNeuroimaging2015,
  title = {Using {{Human Neuroimaging}} to {{Examine Top}}-down {{Modulation}} of {{Visual Perception}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Sprague, Thomas C. and Serences, John T.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {245--274},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_12},
  abstract = {Both univariate and multivariate analysis methods largely have focused on characterizing how measurements from neural firing rates, EEG electrodes, or fMRI voxels change as a function of stimulus parameters or task demands \textendash they focus on characterizing changes in neural signals. However, in cognitive neuroscience we are often interested in how these changes in neural signals collectively modify representations of information. We compare methods whereby activation patterns across entire brain regions can be used to reconstruct representations of information to more traditional univariate and multivariate analysis approaches. We highlight findings using these methods, focusing on how a representation-based analysis approach yields novel insights into how information is encoded, maintained and manipulated under various task demands.},
  file = {/Users/andrew/Dropbox/Zotero/Sprague_Serences2015/Sprague and Serences - 2015 - Using Human Neuroimaging to Examine Top-down Modul.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Analysis,Attention,Decoding,EEG,Encoding,fMRI,Neuroimaging,Reconstruction,Vision,Working memory},
  langid = {english}
}

@incollection{stuphornIntroductionNeuroscientificMethods2015,
  title = {An {{Introduction}} to {{Neuroscientific Methods}}: {{Single}}-Cell {{Recordings}}},
  shorttitle = {An {{Introduction}} to {{Neuroscientific Methods}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Stuphorn, Veit and Chen, Xiaomo},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {113--137},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_6},
  abstract = {This chapter describes the role of single-cell recordings in understanding the mechanisms underlying human cognition. Cognition is a function of the brain, a complex computational network, whose most elementary nodes are made up out of individual neurons. These neurons encode information and influence each other through a dynamically changing pattern of action potentials. For this reason, the activity of neurons in the awake, behaving brain constitutes the most fundamental form of neural data for cognitive neuroscience. This chapter discusses a number of technical issues and challenges of single-cell neurophysiology using a recent project of the authors as an example. We discuss issues such as the choice of an appropriate animal model, the role of psychophysics, technical challenges surrounding the simultaneous recording of multiple neurons, and various methods for perturbation experiments. The chapter closes with a consideration of the challenge that the brain's complexity poses for fully understanding any realistic nervous circuit, and of the importance of conceptual insights and mathematical models in the interpretation of single-cell recordings.},
  file = {/Users/andrew/Dropbox/Zotero/Stuphorn_Chen2015/Stuphorn and Chen - 2015 - An Introduction to Neuroscientific Methods Single.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Action potentials,Animal models,Behavior,Decision making,Electrophysiological recording,Frontal cortex,Nervous circuit,Perturbation experiment,Primate},
  langid = {english}
}

@incollection{turnerConstrainingCognitiveAbstractions2015,
  title = {Constraining {{Cognitive Abstractions Through Bayesian Modeling}}},
  booktitle = {An {{Introduction}} to {{Model}}-{{Based Cognitive Neuroscience}}},
  author = {Turner, Brandon M.},
  editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
  date = {2015},
  pages = {199--220},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4939-2236-9_10},
  abstract = {There are many ways to combine neural and behavioral measures to study cognition. Some ways are theoretical, and other ways are statistical. The predominant statistical approach treats both sources of data as independent and the relationship between the two measures is inferred by way of a (post hoc) regression analysis. In this chapter, we review an alternative approach that allows for flexible modeling of both measures simultaneously. We then explore and elaborate on several of the most important benefits of this modeling approach, and close with a model comparison of the Linear Ballistic Accumulator model and a drift diffusion model on neural and behavioral data.},
  file = {/Users/andrew/Dropbox/Zotero/Turner2015/Turner - 2015 - Constraining Cognitive Abstractions Through Bayesi.pdf},
  isbn = {978-1-4939-2236-9},
  keywords = {Bayesian,cognitive modeling,hierarchical,joint modeling framework},
  langid = {english}
}

@report{vandenberghTutorialConductingInterpreting2019,
  title = {A {{Tutorial}} on {{Conducting}} and {{Interpreting}} a {{Bayesian ANOVA}} in {{JASP}}},
  author = {family=Bergh, given=Don, prefix=van den, useprefix=true and family=Doorn, given=Johnny, prefix=van, useprefix=true and Marsman, Maarten and Draws, Tim and family=Kesteren, given=Erik-Jan, prefix=van, useprefix=true and Derks, Koen and Dablander, Fabian and Gronau, Quentin Frederik and Kucharsk\'y, \v{S}imon and Raj, Akash and Sarafoglou, Alexandra and Voelkel, Jan G. and Stefan, Angelika and Ly, Alexander and Hinne, Max and Matzke, Dora and Wagenmakers, Eric-Jan},
  date = {2019-11-11},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/spreb},
  abstract = {Analysis of variance (ANOVA) is the standard procedure for statistical inference in factorial designs. Typically, ANOVAs are executed using frequentist statistics, where p-values determine statistical significance in an all-or-none fashion. In recent years, the Bayesian approach to statistics is increasingly viewed as a legitimate alternative to the p-value. However, the broad adoption of Bayesian statistics \textendash and Bayesian ANOVA in particular\textendash{} is frustrated by the fact that Bayesian concepts are rarely taught in applied statistics courses. Consequently, practitioners may be unsure how to conduct a Bayesian ANOVA and interpret the results. Herewe provide a guide for executing and interpreting a  Bayesian ANOVA with JASP, an open-source statistical software program with a graphical user interface. We explain the key concepts of the Bayesian ANOVA using twoempirical examples.},
  file = {/Users/andrew/Dropbox/Zotero/van den Bergh et al/2019/van den Bergh et al. - 2019 - A Tutorial on Conducting and Interpreting a Bayesi.pdf},
  type = {preprint}
}

@article{vanrooijFormalizingVerbalTheories2020,
  title = {Formalizing {{Verbal Theories}}},
  author = {family=Rooij, given=Iris, prefix=van, useprefix=true and Blokpoel, Mark},
  date = {2020-09-01},
  journaltitle = {Social Psychology},
  volume = {51},
  pages = {285--298},
  publisher = {{Hogrefe Publishing}},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000428},
  abstract = {. We present a tutorial for formalizing verbal theories of psychological phenomena           \textendash{} social or otherwise. The approach builds on concepts and tools from the mathematics of computation.           We use intuitive examples and illustrate the intrinsic dialectical nature of the formalization process by           presenting dialogues between two fictive characters, called Verbal and             Formal. These characters' conversations and thought experiments serve to highlight           important lessons in theoretical modeling.},
  file = {/Users/andrew/Dropbox/Zotero/van Rooij_Blokpoel/2020/van Rooij and Blokpoel - 2020 - Formalizing Verbal Theories.pdf;/Users/andrew/Zotero/storage/E4QD293I/a000428.html;/Users/andrew/Zotero/storage/KYFYRFCK/a000428.html},
  number = {5}
}

@article{vossDiffusionModelsExperimental2013,
  title = {Diffusion {{Models}} in {{Experimental Psychology}}: {{A Practical Introduction}}},
  shorttitle = {Diffusion {{Models}} in {{Experimental Psychology}}},
  author = {Voss, Andreas and Nagler, Markus and Lerche, Veronika},
  date = {2013-01-01},
  journaltitle = {Experimental Psychology},
  volume = {60},
  pages = {385--402},
  issn = {1618-3169, 2190-5142},
  doi = {10.1027/1618-3169/a000218},
  abstract = {Stochastic diffusion models (Ratcliff, 1978) can be used to analyze response time data from binary decision tasks. They provide detailed information about cognitive processes underlying the performance in such tasks. Most importantly, different parameters are estimated from the response time distributions of correct responses and errors that map (1) the speed of information uptake, (2) the amount of information used to make a decision, (3) possible decision biases, and (4) the duration of nondecisional processes. Although this kind of model can be applied to many experimental paradigms and provides much more insight than the analysis of mean response times can, it is still rarely used in cognitive psychology. In the present paper, we provide comprehensive information on the theory of the diffusion model, as well as on practical issues that have to be considered for implementing the model.},
  file = {/Users/andrew/Dropbox/Zotero/Voss et al/2013/Voss et al. - 2013 - Diffusion Models in Experimental Psychology A Pra.pdf},
  langid = {english},
  number = {6}
}

@article{wagenmakersBayesianBenefitsPragmatic2016,
  title = {Bayesian {{Benefits}} for the {{Pragmatic Researcher}}},
  author = {Wagenmakers, Eric-Jan and Morey, Richard D. and Lee, Michael D.},
  date = {2016-06},
  journaltitle = {Current Directions in Psychological Science},
  volume = {25},
  pages = {169--176},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/0963721416643289},
  abstract = {The practical advantages of Bayesian inference are demonstrated here through two concrete examples. In the first example, we wish to learn about a criminal's IQ: a problem of parameter estimation. In the second example, we wish to quantify and track support in favor of the null hypothesis that Adam Sandler movies are profitable regardless of their quality: a problem of hypothesis testing. The Bayesian approach unifies both problems within a coherent predictive framework, in which parameters and models that predict the data successfully receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more informative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology.},
  file = {/Users/andrew/Dropbox/Zotero/Wagenmakers et al2016/Wagenmakers et al. - 2016 - Bayesian Benefits for the Pragmatic Researcher.pdf},
  langid = {english},
  number = {3}
}

@article{wagenmakersBayesianInferencePsychology2018b,
  title = {Bayesian Inference for Psychology. {{Part II}}: {{Example}} Applications with {{JASP}}},
  shorttitle = {Bayesian Inference for Psychology. {{Part II}}},
  author = {Wagenmakers, Eric-Jan and Love, Jonathon and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Selker, Ravi and Gronau, Quentin F. and Dropmann, Damian and Boutin, Bruno and Meerhoff, Frans and Knight, Patrick and Raj, Akash and family=Kesteren, given=Erik-Jan, prefix=van, useprefix=true and family=Doorn, given=Johnny, prefix=van, useprefix=true and \v{S}m\'ira, Martin and Epskamp, Sacha and Etz, Alexander and Matzke, Dora and family=Jong, given=Tim, prefix=de, useprefix=true and family=Bergh, given=Don, prefix=van den, useprefix=true and Sarafoglou, Alexandra and Steingroever, Helen and Derks, Koen and Rouder, Jeffrey N. and Morey, Richard D.},
  date = {2018-02},
  journaltitle = {Psychonomic Bulletin \& Review},
  volume = {25},
  pages = {58--76},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1323-7},
  file = {/Users/andrew/Dropbox/Zotero/Wagenmakers et al2018/Wagenmakers et al. - 2018 - Bayesian inference for psychology. Part II Exampl.pdf},
  langid = {english},
  number = {1}
}

@article{wagenmakersPracticalSolutionPervasive2007,
  title = {A Practical Solution to the Pervasive Problems of p Values},
  author = {Wagenmakers, Eric-Jan},
  date = {2007-10},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {14},
  pages = {779--804},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03194105},
  file = {/Users/andrew/Dropbox/Zotero/Wagenmakers/2007/Wagenmakers - 2007 - A practical solution to the pervasive problems ofp.pdf},
  langid = {english},
  number = {5}
}

@article{wassersteinASAStatementPValues2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  date = {2016-04-02},
  journaltitle = {The American Statistician},
  volume = {70},
  pages = {129--133},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2016.1154108},
  file = {/Users/andrew/Dropbox/Zotero/Wasserstein_Lazar/2016/Wasserstein and Lazar - 2016 - The ASA Statement on p-Values Context, Process, a.pdf;/Users/andrew/Zotero/storage/HEPS2L8Z/00031305.2016.html},
  number = {2}
}

@article{wilsonTenSimpleRules2019,
  title = {Ten Simple Rules for the Computational Modeling of Behavioral Data},
  author = {Wilson, Robert C and Collins, Anne GE},
  editor = {Behrens, Timothy E},
  date = {2019-11-26},
  journaltitle = {eLife},
  volume = {8},
  pages = {e49547},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.49547},
  abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
  file = {/Users/andrew/Dropbox/Zotero/Wilson_Collins/2019/Wilson and Collins - 2019 - Ten simple rules for the computational modeling of.pdf},
  keywords = {computational modeling,model fitting,reproducibility,validation}
}

@article{wilsonTenSimpleRules2019a,
  title = {Ten Simple Rules for the Computational Modeling of Behavioral Data},
  author = {Wilson, Robert C and Collins, Anne GE},
  editor = {Behrens, Timothy E},
  date = {2019-11-26},
  journaltitle = {eLife},
  volume = {8},
  pages = {e49547},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.49547},
  abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
  file = {/Users/andrew/Dropbox/Zotero/Wilson_Collins/2019/Wilson and Collins - 2019 - Ten simple rules for the computational modeling of2.pdf},
  keywords = {computational modeling,model fitting,reproducibility,validation}
}


